{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLS - DEV\n",
    "\n",
    "Steps:\n",
    "\n",
    "* scene selection - TODO\n",
    "* convert HDF to TIF\n",
    "* creat clear-sky band from QA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import rasterio\n",
    "\n",
    "import nasa_hls\n",
    "\n",
    "from src import configs\n",
    "prjconf = configs.ProjectConfigParser()\n",
    "tilenames = prjconf.get(\"Params\", \"tiles\").split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene selection - TODO\n",
    "\n",
    "**TODO: Find a solution for the following problem. By now we just process all downloaded files.***\n",
    "\n",
    "Here we first downloaded all the data to get the cloud cover and spatial coverage.\n",
    "\n",
    "Now we only want to process scenes which have a cloud cover lower than and a spatial coverage higher than a specific thresold.\n",
    "\n",
    "To keep this easy with snakemake, i.e. to be able to work with simple wildcards, we first create a rule which creates the output directories of the datasets we want to create.\n",
    "Then we can use the output directories as input to the actual processing rule.\n",
    "\n",
    "(**Note:** There is a section about [data-dependent conditional execution](https://snakemake.readthedocs.io/en/stable/snakefiles/rules.html#data-dependent-conditional-execution) which might (?) also be an option.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile = \"32UNU\"\n",
    "\n",
    "basedir__hls_tif = prjconf.get_path(\"Interim\", \"hls\") / tile\n",
    "\n",
    "prjconf = configs.ProjectConfigParser()\n",
    "\n",
    "df_scenes = pd.read_csv(prjconf.get_path(\"Raw\", \"hls_tile_lut\", tile=tile))\n",
    "\n",
    "max_cloud_cover = prjconf.get(\"Params\", \"max_cloud_cover\")\n",
    "min_spatial_cover = prjconf.get(\"Params\", \"min_spatial_cover\")\n",
    "\n",
    "\n",
    "df_scenes_sel = df_scenes[(df_scenes[\"cloud_cover\"] <= float(max_cloud_cover)) & \\\n",
    "                          (df_scenes[\"cloud_cover\"] >= float(min_spatial_cover))]\n",
    "for sid in df_scenes_sel[\"sceneid\"]:\n",
    "    basedir_hls_scene = basedir__hls_tif / sid\n",
    "    if not (basedir_hls_scene).exists():\n",
    "        basedir_hls_scene.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert HDF to TIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An HDF file is a stack of bands. \n",
    "Each HDF will be converted to several single layer TIF files.\n",
    "\n",
    "Input file with wildcards and example:\n",
    "\n",
    "    data/raw/hls/{tile}/{sceneid}.hdf\n",
    "    \n",
    "    data/raw/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4.hdf\n",
    "\n",
    "Output files with wildcards and examples:\n",
    "\n",
    "    data/interim/hls/{tile}/{sceneid}/{sceneid}__{band}.tif\n",
    "    \n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__Red.tif\n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__NIR.tif\n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__SWIR1.tif\n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__SWIR2.tif\n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__QA.tif\n",
    "\n",
    "However it is easier to specify just the output folder as ouput.\n",
    "\n",
    "    directory(\"data/interim/hls/{tile}/{sceneid}\")\n",
    "    \n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4\n",
    "\n",
    "**TODO**: Change ouput to specify all files. How to do this correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule developement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ben/Devel/Projects/classify-hls/data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from nasa_hls import convert_hdf2tiffs\n",
    "\n",
    "hdf_path = prjconf.get_path(\"Raw\", \"hls\") / \"32UNU\" / \"HLS.L30.T32UNU.2018003.v1.4.hdf\" \n",
    "# => in the script: snakemake.input[0]\n",
    "bands = prjconf.get(\"Params\", \"bands\").split(\" \") \n",
    "# => in the script: snakemake.params.bands\n",
    "dir__hls_tif = prjconf.get_path(\"Interim\", \"hls\") / \"32UNU\" / \"HLS.L30.T32UNU.2018003.v1.4\" \n",
    "# => in the script: snakemake.output[0]\n",
    "\n",
    "convert_hdf2tiffs(hdf_path=hdf_path, \n",
    "                  dstdir=Path(dir__hls_tif).parent, \n",
    "                  bands=bands, \n",
    "                  max_cloud_coverage=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat clear-sky band\n",
    "\n",
    "More details on the conversion from QA-layer to clear sky masks can be found in [nasa_hlsâ€™s documentation - Create a clear-sky mask from the QA layer](https://benmack.github.io/nasa_hls/build/html/tutorials/Working_with_HLS_datasets_and_nasa_hls.html#Create-a-clear-sky-mask-from-the-QA-layer).\n",
    "\n",
    "\n",
    "Input file with wildcards and example:\n",
    "\n",
    "    data/interim/hls/{tile}/{sceneid}/{sceneid}__QA.tif\n",
    "    \n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__QA.tif\n",
    "\n",
    "Output files with wildcards and examples:\n",
    "\n",
    "    data/interim/hls/{tile}/{sceneid}/{sceneid}__CLEAR.tif\n",
    "    \n",
    "    data/interim/hls/32UNU/HLS.L30.T32UNU.2018003.v1.4/HLS.L30.T32UNU.2018003.v1.4__CLEAR.tif\n",
    "    \n",
    "Rule developement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing skipped. File exists.\n"
     ]
    }
   ],
   "source": [
    "from nasa_hls import hls_qa_layer_to_mask\n",
    "\n",
    "path__qa = prjconf.get_path(\"Interim\", \"hls\") / \"32UNU\" / \"HLS.L30.T32UNU.2018003.v1.4\" / \"HLS.L30.T32UNU.2018003.v1.4__QA.tif\"\n",
    "# => in the script: snakemake.input[0]\n",
    "path__clear = prjconf.get_path(\"Interim\", \"hls\") / \"32UNU\" / \"HLS.L30.T32UNU.2018003.v1.4\" / \"HLS.L30.T32UNU.2018003.v1.4__CLEAR.tif\"\n",
    "# => in the script: snakemake.output[0]\n",
    "\n",
    "# valid ids from the link above\n",
    "valid = [  0,   4,  16,  20,  32,  36,  48,  52,  64,  68,  80,  84,  96,\n",
    "         100, 112, 116, 128, 132, 144, 148, 160, 164, 176, 180, 192, 196,\n",
    "         208, 212, 224, 228, 240, 244]\n",
    "\n",
    "clear = hls_qa_layer_to_mask(qa_path=path__qa,\n",
    "                             qa_valid=valid,\n",
    "                             keep_255=True,\n",
    "                             mask_path=path__clear,\n",
    "                             overwrite=False)\n",
    "assert clear == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
