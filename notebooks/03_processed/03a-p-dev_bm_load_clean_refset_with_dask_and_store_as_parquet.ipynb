{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load clean reference dataset with dask and store as *parquet* \n",
    "\n",
    "Often even the extracted reference data is too large to fit in memory.\n",
    "An example is if we have many features and a lot of pixels we need work with during modeling or prediction.\n",
    "\n",
    "In this notebook we investigate ways to represent the reference data of multiple tiles by a dask dataframe.\n",
    "\n",
    "The function ``load_extracted_partitions_dask`` we use to create the dask dataframe uses dask delayed to wrap all *npy* files that we have stored by tile and feature to a dask dataframe. \n",
    "\n",
    "Of course, this is not an efficient storage for working with the data in dask. \n",
    "Therefore, we store the data again in the more appropriate parquet format. \n",
    "\n",
    "**IMPOTANT ASSUMPTION:**\n",
    "\n",
    "**No duplicates when we load data from different tiles in one reference dataset (potentiel issue due to overlapping areas).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages, functions and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - \n",
    "# DEFAULT IMPORTS - IN ALL NOTEBOKS\n",
    "from src import configs\n",
    "\n",
    "prjconf = configs.ProjectConfigParser()\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - \n",
    "# NOTEBOOK SPECIFIC IMPORTS\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import shutil\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eobox.raster.extraction import load_extracted_partitions_dask\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    \"\"\"Get a string of the filesize given in bytes in a human readable format.\"\"\"\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the refset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALWAYS USE THESE THREE TOGETHER:\n",
      "id_vectordata  : clc_lte50ha\n",
      "tilenames      : ['32TPT', '32TQT', '32UNU', '32UPU', '32UQU', '33TUN', '33UUP']\n",
      "dir_refset     : /home/ben/Devel/Projects/classify-hls/data/processed/refset01\n"
     ]
    }
   ],
   "source": [
    "refset_id = \"Refset01\"\n",
    "id_vectordata, tilenames = prjconf.get_clean_refset_parameters(refset_id)\n",
    "dir_refset = prjconf.get_path(refset_id, \"rootdir\")\n",
    "\n",
    "print(\"ALWAYS USE THESE THREE TOGETHER:\")\n",
    "print(\"id_vectordata  :\", id_vectordata)\n",
    "print(\"tilenames      :\", tilenames)\n",
    "print(\"dir_refset     :\", dir_refset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the full data and repartition\n",
    "\n",
    "Since we want to store the data again we can adjust - if necessary - the partitions to something better that the tiles (which is the case now). \n",
    "\n",
    "In the (section Repartition to Reduce Overhead of the dask documentation)[https://docs.dask.org/en/latest/dataframe-best-practices.html#repartition-to-reduce-overhead] we can read:\n",
    "\n",
    "*You should aim for partitions that have around 100MB of data each.*\n",
    "\n",
    "Lets see how large our data is and if it makes sense to to change the partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: <class 'dask.dataframe.core.DataFrame'>\n",
      "Number of columns: 62\n",
      "Size of the object  :  56.0B\n",
      "Size of the data    :  297.1MiB\n",
      "Number of partitions: 7\n",
      "Divisions: (0, 661762, 1055590, 2218672, 3159732, 3815927, 4290002, 5025226)\n",
      "Size / shape of partition 0: 39.1MiB / (661762, 62)\n",
      "Size / shape of partition 1: 23.3MiB / (393828, 62)\n",
      "Size / shape of partition 2: 68.8MiB / (1163082, 62)\n",
      "Size / shape of partition 3: 55.6MiB / (941060, 62)\n",
      "Size / shape of partition 4: 38.8MiB / (656195, 62)\n",
      "Size / shape of partition 5: 28.0MiB / (474075, 62)\n",
      "Size / shape of partition 6: 43.5MiB / (735225, 62)\n"
     ]
    }
   ],
   "source": [
    "src_dirs_tiles = {tile:prjconf.get_path(refset_id, \"extracted\", tile=tile) for tile in tilenames}\n",
    "patterns = \"*.npy\"\n",
    "df_full = load_extracted_partitions_dask(src_dir=src_dirs_tiles,\n",
    "                                         global_index_col=\"aux_index_global\", # e.g. \"aux_index_global\",\n",
    "                                         patterns=patterns,\n",
    "                                         verbosity=0)\n",
    "\n",
    "print(\"class:\", df_full.__class__)\n",
    "print(\"Number of columns:\", df_full.shape[1])\n",
    "\n",
    "print(\"Size of the object  : \", sizeof_fmt(sys.getsizeof(df_full)))\n",
    "print(\"Size of the data    : \", sizeof_fmt(df_full.size.compute()))\n",
    "\n",
    "print(\"Number of partitions:\", df_full.npartitions)\n",
    "print(\"Divisions:\", df_full.divisions)\n",
    "\n",
    "for part_n in range(df_full.npartitions):\n",
    "    part_in_mem = df_full.get_partition(part_n).compute()\n",
    "    print(f\"Size / shape of partition {part_n}: {sizeof_fmt(part_in_mem.size)} / {part_in_mem.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want like to change the divisions  it might make sense to keep the pixels of a poylgons together in one partition. \n",
    "This might be better e.g. if we want to plot all the pixels of a polygon together or make some aggregations over polygons.\n",
    "\n",
    "**Note that this only works if the dataframe is already sorted by the column which is used to decide which pixels to be kept together**, here *aux_vector_pid*. In other words, the pixels of a particular polygon should be located in adjacent rows that are not interruped by a pixel of another polygon.\n",
    "\n",
    "Therefore, lets \n",
    "\n",
    "* first get the ideal divisions according to a desired chunk size,\n",
    "\n",
    "* then all potential divisions according to the criteria above and \n",
    "\n",
    "* then get the final divisions by selecting from the potential divisions those which are closest to the ideal divisions. \n",
    "\n",
    "Let's say we want around 500.000 pixels per chunk to have at least as many chunks as we have cores (assuming 8) - even though the will be less than the 100 MB suggested in the best practice. (Nut sure if this makes sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideal divisions:\n",
      " [0, 500000, 1000000, 1500000, 2000000, 2500000, 3000000, 3500000, 4000000, 4500000, 5000000]\n"
     ]
    }
   ],
   "source": [
    "# ideal divisions start\n",
    "partition_size = 500000\n",
    "max_index = df_full.index.max().compute() # assuming gap-free sequential indices\n",
    "ideal_divisions = list(range(0, max_index + 1, partition_size)) #  + [max_index]\n",
    "print(\"Ideal divisions:\\n\", ideal_divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential divisions:\n",
      " [      0     479     858 ... 5024098 5024382 5024769]\n"
     ]
    }
   ],
   "source": [
    "# find elements / rows where a new polygon start\n",
    "start_of_newpolygon = (df_full[\"aux_vector_pid\"].diff() != 0).compute()\n",
    "potential_divisions = start_of_newpolygon[start_of_newpolygon].index.values\n",
    "print(\"Potential divisions:\\n\", potential_divisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final divisions:\n",
      " [0, 499998, 999936, 1500051, 2000040, 2499889, 3000140, 3499782, 3999883, 4499913, 5025226]\n",
      "Index difference between the ideal and the final divisions:\n",
      " [0, -2, -64, 51, 40, -111, 140, -218, -117, -87, 25226]\n"
     ]
    }
   ],
   "source": [
    "final_divisions = []\n",
    "for idiv in ideal_divisions:\n",
    "    final_divisions.append(potential_divisions[np.argmin(abs(potential_divisions - idiv))])\n",
    "# make the closing index. \n",
    "# create a new devision if the number of rows to the last division half of the partition size.\n",
    "# else overwrite the last element   \n",
    "if (max_index - final_divisions[-1]) > partition_size / 2:\n",
    "    final_divisions.append(max_index)\n",
    "else:\n",
    "    final_divisions[-1] = max_index\n",
    "    \n",
    "print(\"Final divisions:\\n\", final_divisions)\n",
    "print(\"Index difference between the ideal and the final divisions:\\n\", \n",
    "      [fdiv - idiv for fdiv, idiv in zip(final_divisions, ideal_divisions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known_divisions :  True\n",
      "npartitions     :  10\n",
      "Size / shape of partition 0: 29.6MiB / (499998, 62)\n",
      "Size / shape of partition 1: 29.6MiB / (499938, 62)\n",
      "Size / shape of partition 2: 29.6MiB / (500115, 62)\n",
      "Size / shape of partition 3: 29.6MiB / (499989, 62)\n",
      "Size / shape of partition 4: 29.6MiB / (499849, 62)\n",
      "Size / shape of partition 5: 29.6MiB / (500251, 62)\n",
      "Size / shape of partition 6: 29.5MiB / (499642, 62)\n",
      "Size / shape of partition 7: 29.6MiB / (500101, 62)\n",
      "Size / shape of partition 8: 29.6MiB / (500030, 62)\n",
      "Size / shape of partition 9: 31.1MiB / (525314, 62)\n"
     ]
    }
   ],
   "source": [
    "df_full_part = df_full.repartition(divisions=final_divisions)\n",
    "print(\"known_divisions : \", df_full_part.known_divisions)\n",
    "print(\"npartitions     : \", df_full_part.npartitions)\n",
    "for part_n in range(df_full_part.npartitions):\n",
    "    part_in_mem = df_full_part.get_partition(part_n).compute()\n",
    "    print(f\"Size / shape of partition {part_n}: {sizeof_fmt(part_in_mem.size)} / {part_in_mem.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the polygon IDs are really mutually exclusive between the partitions.\n",
    "In other words, we want that there are no unique polygon IDs common to any partition pair.\n",
    "\n",
    "If the following code runs through without any error or notification everything is as assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part_a in range(df_full_part.npartitions):\n",
    "    for part_b in range(df_full_part.npartitions):\n",
    "        if part_a == part_b:\n",
    "            continue\n",
    "        else:\n",
    "            uniques_part_A = df_full_part.get_partition(part_a)[\"aux_vector_pid\"].unique().compute()\n",
    "            uniques_part_B = df_full_part.get_partition(part_b)[\"aux_vector_pid\"].unique().compute()\n",
    "            intersection = set(uniques_part_A).intersection(set(uniques_part_B))\n",
    "            if len(intersection) != 0:\n",
    "                raise Exception(f\"Common polygon IDs in partitions {part_a} and {part_b}: {intersection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new partitions as parquet files\n",
    "\n",
    "Lets follow the [dask dataframe best practices](https://docs.dask.org/en/latest/dataframe-best-practices.html#store-data-in-apache-parquet-format) and store our data as parquet.\n",
    "\n",
    "However, we separate the auxiliary data from the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ben/Devel/Projects/classify-hls/data/processed/refset01/optimized_refsets/parquet/aux\n",
      "/home/ben/Devel/Projects/classify-hls/data/processed/refset01/optimized_refsets/parquet/features_bands_vts4w\n",
      "/home/ben/Devel/Projects/classify-hls/data/processed/refset01/optimized_refsets/parquet/aux_u_features_bands_vts4w\n"
     ]
    }
   ],
   "source": [
    "overwrite = False\n",
    "\n",
    "# # all as read in ordered alphabetically\n",
    "aux_cols = [col for col in df_full_part.columns if 'aux_' in col]\n",
    "vts_feature_cols = [col for col in df_full_part.columns if '_vts4w_' in col]\n",
    "\n",
    "# # save\n",
    "dir_parquet_aux = dir_refset / \"optimized_refsets\" / \"parquet\" / \"aux\"\n",
    "if not dir_parquet_aux.exists():\n",
    "    dir_parquet_aux.mkdir(exist_ok=True, parents=True)\n",
    "    df_full_part[aux_cols].to_parquet(str(dir_parquet_aux))\n",
    "print(dir_parquet_aux)\n",
    "\n",
    "dir_parquet_features_bands_vts4w = dir_refset / \"optimized_refsets\" / \"parquet\" / \"features_bands_vts4w\"\n",
    "if not dir_parquet_features_bands_vts4w.exists():\n",
    "    dir_parquet_features_bands_vts4w.mkdir(exist_ok=True, parents=True)\n",
    "    df_full_part[vts_feature_cols].to_parquet(str(dir_parquet_features_bands_vts4w))\n",
    "print(dir_parquet_features_bands_vts4w)\n",
    "\n",
    "dir_parquet = dir_refset / \"optimized_refsets\" / \"parquet\" / \"aux_u_features_bands_vts4w\"\n",
    "if not dir_parquet.exists():\n",
    "    dir_parquet.mkdir(exist_ok=True, parents=True)\n",
    "    df_full_part.to_parquet(str(dir_parquet))\n",
    "print(dir_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read parquet data\n",
    "\n",
    "Read and combine dask dataframes from parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = dd.read_parquet(str(dir_parquet_aux))\n",
    "df_features = dd.read_parquet(str(dir_parquet_features_bands_vts4w))\n",
    "\n",
    "df_parquet = dd.concat([df_aux, df_features], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
