{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features - statistical metrics - DEV\n",
    "\n",
    "This code processes all variables at once. \n",
    "\n",
    "Advantage: The QA layer has to be loaded only once\n",
    "\n",
    "Disadvantage: Parallelization and check for / skip over esults of existing single bands with Snakemake. \n",
    "\n",
    "**TODO**: Create a Snakemake task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['32UNU', '32UPU', '32UQU', '33UUP', '32TPT', '32TQT', '33TUN']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - \n",
    "# DEFAULT IMPORTS - IN ALL NOTEBOKS\n",
    "from src import configs\n",
    "\n",
    "prjconf = configs.ProjectConfigParser()\n",
    "\n",
    "# - - - - - - - - - - - - - - - - - - - - \n",
    "# NOTEBOOK SPECIFIC IMPORTS\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from eobox.raster import cube\n",
    "from eobox.raster import gdalutils\n",
    "\n",
    "tilenames = prjconf.get(\"Params\", \"tiles\").split(\" \")\n",
    "\n",
    "tilenames =['32UNU', '32UPU', '32UQU', '33UUP', '32TPT', '32TQT', '33TUN']\n",
    "\n",
    "tilenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "### Parameters for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scoll_name  : scoll01\n",
      "variables   : ['Red', 'NIR', 'SWIR1', 'SWIR2']\n",
      "qa          : CLEAR\n"
     ]
    }
   ],
   "source": [
    "scoll_id = 1\n",
    "scoll_name = f\"scoll{scoll_id:02d}\"\n",
    "\n",
    "variables = [\"Red\", \"NIR\", \"SWIR1\", \"SWIR2\"]\n",
    "qa = \"CLEAR\"\n",
    "qa_valid = [1]\n",
    "\n",
    "print(f\"{'scoll_name':12s}: {scoll_name}\")\n",
    "print(f\"{'variables':12s}: {variables}\")\n",
    "print(f\"{'qa':12s}: {qa}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "\n",
    "In Snakemake the list would be created by wildcard catching the tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'32UNU': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/32UNU/df_scoll01.csv'),\n",
       " '32UPU': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/32UPU/df_scoll01.csv'),\n",
       " '32UQU': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/32UQU/df_scoll01.csv'),\n",
       " '33UUP': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/33UUP/df_scoll01.csv'),\n",
       " '32TPT': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/32TPT/df_scoll01.csv'),\n",
       " '32TQT': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/32TQT/df_scoll01.csv'),\n",
       " '33TUN': PosixPath('/home/ben/Devel/Projects/classify-hls/data/raw/scene_collections/33TUN/df_scoll01.csv')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scolls = {tile: (prjconf.get_path(\"Raw\", \"scene_colls\") / tile / f\"df_{scoll_name}.csv\") for tile in tilenames}\n",
    "scolls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    dst_paths = {}\n",
    "    for tile in tilenames:\n",
    "        dst_paths[tile] = prjconf.get_paths_features_stats_regular_raster(scoll_name, tile, variables, metrics, as_dict=True)\n",
    "        for var in variables:\n",
    "            print(f\"First and last file (of {len(dst_paths[tile][var])}) of {(tile + ' ' + var)}\")\n",
    "            print(\"   \" + dst_paths[tile][var][0])\n",
    "            print(\"   \" + dst_paths[tile][var][-1])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "32UNU\n",
      "/home/ben/Devel/Projects/classify-hls/data/processed/L3/raster/32UNU/scoll01/32unu__scoll01__stats__{metric}__{var}.vrt\n"
     ]
    }
   ],
   "source": [
    "for tile in tilenames:\n",
    "    print(\"*\" * 100)\n",
    "    print(tile)\n",
    "\n",
    "    scoll_layers = prjconf.get_layer_df_of_scene_collection(scoll_name, variables + [qa], tile)\n",
    "    assert len(scoll_layers[\"tile\"].unique()) == 1\n",
    "    assert len(scoll_layers[\"product\"].unique()) == 1  \n",
    "    # here L30 & S30 would also make sense but then we need to change the product string below\n",
    "    # tile = scoll_layers[\"tile\"].unique()[0]\n",
    "    # product = scoll_layers[\"product\"].unique()[0]\n",
    "\n",
    "    dst_pattern = prjconf.get_paths_features_stats_regular_raster(scoll_name, tile, variables, as_dict=True, return_patter=True)\n",
    "    print(dst_pattern)\n",
    "    break\n",
    "    ## Run Task\n",
    "\n",
    "    scoll = cube.EOCubeSceneCollection(df_layers=scoll_layers, \n",
    "                                       chunksize=2**9, \n",
    "                                       variables=variables, \n",
    "                                       qa=qa, \n",
    "                                       qa_valid=qa_valid \n",
    "                                      )\n",
    "\n",
    "    scoll.create_statistical_metrics(\n",
    "        percentiles=[.05, .1, .25, .5, .75, .9, .95],\n",
    "        dst_pattern=dst_pattern,\n",
    "        dtypes=\"int16\",\n",
    "        compress='lzw',\n",
    "        nodata=None,\n",
    "        num_workers=6)\n",
    "\n",
    "    ## Create VRTs\n",
    "    # Create a time series layer stack (VRT) for each variable. \n",
    "\n",
    "    dst_dir = prjconf.get_path(\"Processed\", \"raster\", tile=tile) / scoll_name\n",
    "    dst_dir_vsts_stack = prjconf.get_path(\"Processed\", \"raster\", tile=tile) / \"VRTs\" / \"ts_per_band\" / tile / f\"{scoll_name}\"\n",
    "    dst_dir_vsts_stack.mkdir(parents=True, exist_ok=True)\n",
    "    dst_dir_vsts_stack\n",
    "\n",
    "    for var in scoll.variables:\n",
    "        input_file_list = list(list(Path(dst_dir).glob(f\"*stats__{var}*.vrt\")))\n",
    "        input_file_list = np.sort(input_file_list)\n",
    "        output_file = Path(dst_dir_vsts_stack) / f\"{tile.lower()}__{scoll_name}__stats__{var}.vrt\"\n",
    "        print(output_file)\n",
    "        gdalutils.buildvrt(input_file_list, output_file, relative=True, separate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
